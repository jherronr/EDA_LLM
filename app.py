import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import io
import os
from langchain_groq import ChatGroq
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain

# --- CONFIGURACI√ìN DE LA P√ÅGINA Y ESTILOS ---
st.set_page_config(page_title="An√°lisis de Datos con LLM", layout="wide", initial_sidebar_state="collapsed")
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (10, 6)

# --- T√çTULO Y DESCRIPCI√ìN ---
st.title('üìä Herramienta de An√°lisis de Datos y Asistente LLM')
st.write('Sube un archivo CSV para generar un an√°lisis exploratorio completo y obtener una conclusi√≥n generada por un LLM.')

# --- INICIALIZACI√ìN DEL ESTADO DE LA SESI√ìN ---
# Esto es crucial para mantener los datos y mensajes entre interacciones.
if "messages" not in st.session_state:
    st.session_state.messages = []
if "df" not in st.session_state:
    st.session_state.df = None
if "eda_summary" not in st.session_state:
    st.session_state.eda_summary = None
if "llm_conclusion" not in st.session_state:
    st.session_state.llm_conclusion = None

# --- FUNCIONES AUXILIARES ---

def generate_eda_summary(df: pd.DataFrame) -> str:
    """
    Genera un resumen textual del An√°lisis Exploratorio de Datos (EDA).
    Este resumen ser√° el contexto principal para el LLM.
    """
    buffer = io.StringIO()
    df.info(buf=buffer)
    info_str = buffer.getvalue()
    
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
    
    summary = f"""
    ### Resumen del An√°lisis Exploratorio de Datos (EDA)

    #### 1. Informaci√≥n General y Tipos de Datos:
    ```
    {info_str}
    ```

    #### 2. Estad√≠sticas Descriptivas (Variables Num√©ricas):
    ```
    {df.describe().T.to_string()}
    ```

    #### 3. Conteo de Valores Nulos por Columna:
    ```
    {df.isnull().sum().to_string()}
    ```

    #### 4. Total de Filas Duplicadas:
    **{df.duplicated().sum()}**

    """
    if len(numeric_cols) > 1:
        summary += f"""
    #### 5. Matriz de Correlaci√≥n (Variables Num√©ricas):
    ```
    {df[numeric_cols].corr().to_string()}
    ```
    """
    return summary

def get_llm_conclusion(eda_summary: str) -> str:
    """
    Llama al LLM para generar una conclusi√≥n basada en el resumen del EDA.
    """
    try:
        if "GROQ_API_KEY" not in os.environ or not os.environ["GROQ_API_KEY"]:
            return "‚ö†Ô∏è **Advertencia:** La clave de API de Groq no est√° configurada. No se puede generar la conclusi√≥n."

        llm = ChatGroq(
            temperature=0.1,
            groq_api_key=os.environ["GROQ_API_KEY"],
            model_name="gemma2-9b-it"
        )
        
        system_prompt = (
            "Eres un analista de datos experto y comunicador excepcional. "
            "Tu tarea es analizar el siguiente resumen de un An√°lisis Exploratorio de Datos (EDA) y redactar una conclusi√≥n clara y concisa en espa√±ol. "
            "Tu conclusi√≥n debe ser f√°cil de entender para una audiencia no t√©cnica. "
            "**Estructura tu respuesta de la siguiente manera:**\n"
            "1.  **Calidad de los Datos:** Comenta sobre valores nulos, duplicados y tipos de datos. ¬øEl dataset est√° limpio o requiere preparaci√≥n?\n"
            "2.  **Principales Hallazgos:** Describe las tendencias, patrones o distribuciones m√°s importantes que observas en las estad√≠sticas.\n"
            "3.  **Correlaciones Relevantes:** Si hay una matriz de correlaci√≥n, menciona las correlaciones fuertes (positivas o negativas) y lo que podr√≠an significar.\n"
            "4.  **Recomendaciones:** Basado en el an√°lisis, sugiere los siguientes pasos o √°reas de inter√©s para una investigaci√≥n m√°s profunda.\n\n"
            "**IMPORTANTE:** Basa tu conclusi√≥n √öNICAMENTE en el resumen del EDA proporcionado o la base de datos entregada por el usuario. No inventes informaci√≥n. No generes c√≥digo. No respondas temas que no est√©n directamente relacionados con la base de datos entregada."
        )
        
        human_prompt = "Aqu√≠ est√° el resumen del EDA:\n\n---\n\n{eda_summary}\n\n---\n\nPor favor, genera la conclusi√≥n."
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", human_prompt)
        ])
        
        llm_chain = LLMChain(prompt=prompt_template, llm=llm)
        response = llm_chain.invoke({"eda_summary": eda_summary})['text']
        return response

    except Exception as e:
        return f"‚ùå Ocurri√≥ un error al contactar al modelo LLM: {e}"

# --- CARGA DEL ARCHIVO ---
uploaded_file = st.file_uploader("Elige un archivo CSV", type="csv")

if uploaded_file is not None and st.session_state.df is None:
    try:
        df = pd.read_csv(uploaded_file)
        st.session_state.df = df
        st.success("‚úÖ Archivo cargado exitosamente. Generando an√°lisis...")
        
        # Generar y guardar el resumen del EDA y la conclusi√≥n del LLM una sola vez.
        st.session_state.eda_summary = generate_eda_summary(df)
        with st.spinner('ü§ñ El asistente LLM est√° analizando los datos para generar una conclusi√≥n...'):
            st.session_state.llm_conclusion = get_llm_conclusion(st.session_state.eda_summary)

    except Exception as e:
        st.error(f"Ocurri√≥ un error al procesar el archivo: {e}")
        st.session_state.df = None # Resetear en caso de error

# --- VISUALIZACI√ìN DEL AN√ÅLISIS (si el df existe) ---
if st.session_state.df is not None:
    df = st.session_state.df
    
    # --- Pesta√±as para organizar el contenido ---
    tab1, tab2, tab3 = st.tabs(["üìÑ Resumen y Estad√≠sticas", "üìà Visualizaciones", "ü§ñ Conclusi√≥n y Chat"])

    with tab1:
        st.header('üîç Resumen de los Datos')
        
        st.subheader('Informaci√≥n General de las Columnas')
        buffer = io.StringIO()
        df.info(buf=buffer)
        st.text(buffer.getvalue())
        
        st.subheader('Primeras 5 Filas')
        st.dataframe(df.head())
        
        st.subheader('Estad√≠sticas Descriptivas')
        st.dataframe(df.describe().T)
        
        st.subheader('Valores Nulos y Duplicados')
        col1, col2 = st.columns(2)
        with col1:
            st.write("Conteo de valores nulos:")
            st.dataframe(df.isnull().sum())
        with col2:
            st.write(f"Total de filas duplicadas:")
            st.metric("Duplicados", df.duplicated().sum())

    with tab2:
        st.header('üìà Visualizaciones del EDA')
        
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
        cat_cols = df.select_dtypes(include='object').columns
        
        if not numeric_cols.empty:
            st.subheader('Distribuci√≥n de Variables Num√©ricas')
            for col in numeric_cols:
                fig, ax = plt.subplots(1, 2, figsize=(15, 5))
                sns.histplot(df[col], kde=True, ax=ax[0])
                ax[0].set_title(f'Histograma de {col}')
                sns.boxplot(x=df[col], ax=ax[1])
                ax[1].set_title(f'Boxplot de {col}')
                st.pyplot(fig)

        if not cat_cols.empty:
            st.subheader('Distribuci√≥n de Variables Categ√≥ricas')
            for col in cat_cols:
                fig, ax = plt.subplots()
                sns.countplot(y=df[col], order=df[col].value_counts().index, ax=ax)
                ax.set_title(f'Conteo de {col}')
                st.pyplot(fig)
        
        if len(numeric_cols) > 1:
            st.subheader('Matriz de Correlaci√≥n')
            fig, ax = plt.subplots()
            corr_matrix = df[numeric_cols].corr()
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)
            ax.set_title('Matriz de Correlaci√≥n')
            st.pyplot(fig)

    with tab3:
        st.header('ü§ñ Conclusi√≥n del Asistente LLM')
        if st.session_state.llm_conclusion:
            st.markdown(st.session_state.llm_conclusion)
        else:
            st.info("La conclusi√≥n del LLM aparecer√° aqu√≠ despu√©s de cargar los datos.")

        st.markdown("---")
        st.header('üí¨ Chatea sobre tus Datos')
        st.write('Haz preguntas espec√≠ficas sobre el an√°lisis o los datos.')

        # Mostrar mensajes del chat
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # Input del usuario para el chat
        if prompt := st.chat_input("Ej: ¬øCu√°l es la correlaci√≥n entre la columna A y B?"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("Pensando..."):
                    try:
                        if "GROQ_API_KEY" not in os.environ or not os.environ["GROQ_API_KEY"]:
                            st.warning("La clave de API de Groq no est√° configurada.")
                            continue

                        llm = ChatGroq(temperature=0, model_name="gemma2-9b-it")
                        
                        system_prompt_qa = (
                            "Eres un asistente de an√°lisis de datos. Se te ha proporcionado un resumen del EDA y el dataset completo. "
                            "Tu tarea es responder preguntas espec√≠ficas del usuario sobre los datos. "
                            "Utiliza la informaci√≥n del resumen y del dataset para formular tus respuestas. "
                            "No respondas preguntas que est√©n por fuera de la base de datos o que no sean del tema de la base de datos"
                            "S√© directo y conciso. No intentes generar gr√°ficos ni c√≥digo.\n\n"
                            f"--- RESUMEN DEL EDA ---\n{st.session_state.eda_summary}\n\n"
                            f"--- DATASET (primeras 100 filas) ---\n{df.to_string()}\n---"
                        )
                        
                        prompt_template_qa = ChatPromptTemplate.from_messages([
                            ("system", system_prompt_qa),
                            ("human", "{user_question}")
                        ])
                        
                        llm_chain = LLMChain(prompt=prompt_template_qa, llm=llm)
                        response = llm_chain.invoke({"user_question": prompt})['text']
                        st.markdown(response)
                        st.session_state.messages.append({"role": "assistant", "content": response})

                    except Exception as e:
                        st.error(f"Ocurri√≥ un error al llamar al LLM: {e}")

